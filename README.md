
# Introduction

<p>This repository is the code implementation of the paper "<I>Fusion the Power from Citations: Improve your influence by Integrating Information from References</I>" in TheWebConference 2022</p>


The directory of the project is as follows:

```python
├── 0.datasets
│   ├── embedding
│   ├── mapping
│   ├── processed
│   │   ├── paper_detail_v3.csv
│   └── raw
├── 1.preprocess
│   ├── 1.convert2csv.py
│   ├── 2.merge_paper_partitions.py
│   ├── 3.reference_citations.py
│   ├── 4.tidy_data.py
│   └── 5.reindex.py
├── 2.calculate_acc
│   ├── 1.author_detail.py
│   ├── 2.paper_citation_reduce.py
│   ├── 3.paper_citation_seq.py
│   ├── 4.calculate_acc.py
│   └── 5.label_acc.py
├── 3.metapath2vec
│   ├── 1.AMiner2Vec.py
│   ├── 2.paper_embedding.py
│   └── description.py
├── 4.models
│   ├── 1.aminer_graph.py
│   ├── 2.edge_table.py
│   ├── 3.RD-GAT.py
│   └── 4.Plain_GAT.py
├── 5.evaluation
├── README.md
└── utility
    └── publication_analysis.py
```

## Preprocess

<p>Convert2csv: convert the raw data into dataframes separately (50000 records per csv file), the schema is (“#index”, “#*”, “#@”, “#o”, “#t”, “#c”, “paper_reference”, “#!”).
#index: Id for each paper
#*: title
#@: authors (separated by semicolons)
#o: affilations (separated by semicolons)
#t: year
#c: publication venue
Paper_reference: [id_r1,id_ r2, id_r3, …]
#!: abstract</p>


<p>merge_paper_partitions.py: this function will generate the whole csv file of the academic network. (paper_detail_v1.csv). </p>


<p>Referece&Citation.py: This file adds the reference_num and citation_num information to the detail.csv (paper_detail_v2.csv)</p>

<p>Tidy_Data.py: This file expurgate the deficient papers which lacks title, abstract or author information and delete the papers which are isolated, meaning that there are no references or citations of them. (paper_detail_v3.py)</p>


## Calculate_ACC

<p>author_detail.py: 
primary_author_info function: label the author according to their position, A1, B1 or C1 respectively.
result: datasets/processed/author_detail.csv</p>

<p>paper_citation_reduce.py: 
calculate the citation count since the paper has been published, the citation count will be accumulated every year.
result: datasets/processed/paper_citation_reduce.csv</p>

<p>paper_citation_seq.py: 
convert the [1998, 1999, …, 2014] sequence data into the [cited_0, cited_1, …, cited_10] form.</p>


## Metapath2Vec

<p>AMiner2Vec.py: 
Mapping function: preserve the index mapping in the author_mapping, venue_mapping and paper_mapping files;

Relationship function: extract the author-writes-paper and venue-publishes-paper relations;

hgraph: the heterogeneous graph data object, the index of each type of node starts at 0;

model: the MetaPath2Vec object generated by the PyGeometric;

train step: according to the original paper, the model will be converged after five epochs.</p>


<p>Paper_Embedding.py:

Firstly, load the trained model; then, detect the sequence of the node types, in our setting, they are [“author”, “paper”, “venue”]; 

In the get_embedding function, the model.start() and model.end() will receive the node_type as parameter and return the index of each type of node.

Finally, save the paper_embedding (torch.Tensor) into the /datasets/embedding directory.</p>


## Models

<p>aminer_graph.py:
train_test_split function: split the entire graph into 70% train and 30% test dataset.

get_edge_index function: convert the csv file into the mapped [source, target] entry;

get_x function: load the paper’s representation trained by metapath2vec.

get_y function: use the entire datasets (1998 - 2014) label_5 as the ground truth label, and the positive rate is 41.02%.

aminer object: AMiner paper’s citation network.</p>
